{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Based Web Scraping of Clark U websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Patch asyncio for Jupyter\n",
    "\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional  # Added missing import\n",
    "\n",
    "from scrapegraphai.graphs import DepthSearchGraph\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Set TOKENIZERS_PARALLELISM explicitly to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_key = os.getenv('GEMINI_KEY')\n",
    "# hf_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Define the configuration for the graph\n",
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": openai_key,\n",
    "        \"model\": \"openai/gpt-4o-mini\",\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"headless\": True,\n",
    "    \"depth\": 200,              \n",
    "    \"only_inside_links\": True  # Only follow links inside the same domain\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of approximately 'chunk_size' words.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def process_page(page, counter):\n",
    "    \"\"\"\n",
    "    Given a page (dict with keys like 'title', 'url', 'content') extracted by the LLM,\n",
    "    process the content and return a list of JSON entries.\n",
    "    \n",
    "    The LLM typically provides more structured content that needs different handling\n",
    "    than raw HTML.\n",
    "    \"\"\"\n",
    "    title = page.get(\"title\", \"\")\n",
    "    url = page.get(\"url\", \"\")\n",
    "    content = page.get(\"content\", \"\")\n",
    "    \n",
    "    # Check if content is already a string\n",
    "    if not isinstance(content, str):\n",
    "        try:\n",
    "            # Try to convert to string\n",
    "            content = str(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert content to string for {url}: {e}\")\n",
    "            content = \"\"\n",
    "    \n",
    "    # Extract text from content - LLM may provide already cleaned content\n",
    "    # so we may not need BeautifulSoup parsing here\n",
    "    clean_text = content\n",
    "    \n",
    "    # If content looks like HTML, try to clean it\n",
    "    if \"<\" in content and \">\" in content and (\"<html\" in content.lower() or \"<body\" in content.lower()):\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content with BeautifulSoup for {url}: {e}\")\n",
    "            print(f\"Using original content instead\")\n",
    "    \n",
    "    # Split the cleaned text into chunks\n",
    "    chunks = chunk_text(clean_text, chunk_size=300)\n",
    "    entries = []\n",
    "    \n",
    "    # If no chunks were created, create at least one entry with whatever content we have\n",
    "    if not chunks and (title or url):\n",
    "        entry = {\n",
    "            \"id\": str(counter),\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"text\": clean_text if clean_text else \"No content extracted\"\n",
    "        }\n",
    "        entries.append(entry)\n",
    "        counter += 1\n",
    "    else:\n",
    "        for chunk in chunks:\n",
    "            entry = {\n",
    "                \"id\": str(counter),\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"text\": chunk\n",
    "            }\n",
    "            entries.append(entry)\n",
    "            counter += 1\n",
    "    \n",
    "    # Print a summary\n",
    "    print(f\"Processed URL: {url} - Created {len(entries)} chunks\")\n",
    "    \n",
    "    return entries, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_depth_scrape(source_url, config):\n",
    "    \"\"\"\n",
    "    Uses DepthSearchGraph to recursively scrape a given source URL.\n",
    "    The prompt instructs the LLM to extract the page title and all textual content.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Extract all useful textual content from the webpage, including its title. \"\n",
    "        \"Extract specific metadata (e.g., faculty names, research interests, course details) if present. \"\n",
    "        \"Preserve sections like headers or subtitles that can help structure the content. \"\n",
    "        \"Identify and include citation-like metadata (e.g., source URLs) for later reference. \"\n",
    "        \"Recursively follow internal links up to the specified depth. \"\n",
    "        \"Return each page as a JSON object with keys 'title', 'url', and 'content'.\"\n",
    "    )\n",
    "    depth_graph = DepthSearchGraph(\n",
    "        prompt=prompt,\n",
    "        source=source_url,\n",
    "        config=config\n",
    "    )\n",
    "    result = depth_graph.run()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced URL Collection and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(url):\n",
    "    \"\"\"Extract domain from URL\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc\n",
    "\n",
    "def extract_links(html, base_url):\n",
    "    \"\"\"Extract all links from HTML content\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    domain = get_domain(base_url)\n",
    "    links = []\n",
    "    \n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        # Convert relative URLs to absolute\n",
    "        if href.startswith('/'):\n",
    "            href = f\"https://{domain}{href}\"\n",
    "        elif not href.startswith(('http://', 'https://')):\n",
    "            # Skip javascript links, anchors, etc.\n",
    "            continue\n",
    "        \n",
    "        # Only include links from the same domain\n",
    "        if get_domain(href) == domain:\n",
    "            links.append(href)\n",
    "    \n",
    "    return list(set(links))  # Remove duplicates\n",
    "\n",
    "def fetch_url(url):\n",
    "    \"\"\"Fetch URL content with error handling\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_content(html, url):\n",
    "    \"\"\"Extract title and content from HTML with improved handling\"\"\"\n",
    "    if not html:\n",
    "        return None, None\n",
    "        \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Get title\n",
    "    title = \"\"\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "    else:\n",
    "        # Try h1 tag if title not found\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            title = h1_tag.get_text(strip=True)\n",
    "    \n",
    "    # Extract main content - improved approach\n",
    "    # First remove navigation, headers, footers, scripts, and ads\n",
    "    for element in soup.find_all(['nav', 'header', 'footer', 'script', 'style', 'noscript', \n",
    "                                  'iframe', 'svg', 'aside', 'form']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Try common content containers\n",
    "    main_content_tags = [\n",
    "        # ID-based selectors\n",
    "        {'tag': 'div', 'attrs': {'id': ['content', 'main-content', 'mainContent', 'main']}},\n",
    "        {'tag': 'main', 'attrs': {}},\n",
    "        {'tag': 'article', 'attrs': {}},\n",
    "        # Class-based selectors\n",
    "        {'tag': 'div', 'attrs': {'class': ['content', 'main-content', 'main', 'article', 'post']}},\n",
    "    ]\n",
    "    \n",
    "    main_content = None\n",
    "    \n",
    "    # Try each potential content container\n",
    "    for selector in main_content_tags:\n",
    "        tag = selector['tag']\n",
    "        attrs = selector['attrs']\n",
    "        \n",
    "        # Handle multi-value attributes\n",
    "        for attr_name, attr_values in attrs.items():\n",
    "            if isinstance(attr_values, list):\n",
    "                for value in attr_values:\n",
    "                    elements = soup.find_all(tag, {attr_name: lambda x: x and value in x.split() if x else False})\n",
    "                    if elements:\n",
    "                        # Use the largest element by text length\n",
    "                        main_content = max(elements, key=lambda e: len(e.get_text(strip=True)))\n",
    "                        break\n",
    "            if main_content:\n",
    "                break\n",
    "        \n",
    "        # No need to continue if we found content\n",
    "        if main_content:\n",
    "            break\n",
    "    \n",
    "    # If no specific content container found, use body\n",
    "    if not main_content:\n",
    "        main_content = soup.body or soup\n",
    "    \n",
    "    # Extract content by sections\n",
    "    sections = []\n",
    "    \n",
    "    # Extract headers and their content\n",
    "    headers = main_content.find_all(['h1', 'h2', 'h3', 'h4'])\n",
    "    \n",
    "    if headers:\n",
    "        for header in headers:\n",
    "            header_text = header.get_text(strip=True)\n",
    "            if not header_text:\n",
    "                continue\n",
    "            \n",
    "            content_parts = []\n",
    "            \n",
    "            # Get all siblings until next header or end\n",
    "            sibling = header.next_sibling\n",
    "            while sibling and sibling.name not in ['h1', 'h2', 'h3', 'h4']:\n",
    "                if hasattr(sibling, 'get_text'):\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        content_parts.append(text)\n",
    "                sibling = sibling.next_sibling\n",
    "                \n",
    "            section_content = \" \".join(content_parts)\n",
    "            sections.append({\n",
    "                'header': header_text,\n",
    "                'text': section_content if section_content else \"No content in this section\"\n",
    "            })\n",
    "    \n",
    "    # If no headers found or no content under headers, use paragraphs\n",
    "    if not sections or not any(section.get('text') for section in sections):\n",
    "        paragraphs = main_content.find_all('p')\n",
    "        if paragraphs:\n",
    "            all_text = \" \".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            sections.append({'text': all_text})\n",
    "        else:\n",
    "            # Last resort: get all text\n",
    "            all_text = main_content.get_text(\" \", strip=True)\n",
    "            sections.append({'text': all_text})\n",
    "    \n",
    "    # Format as JSON string\n",
    "    content_dict = {'sections': sections}\n",
    "    return title, str(content_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution with Enhanced Link Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with root URL: https://www.clarku.edu/\n",
      "Found 105 links on homepage\n",
      "Processing URL 1/50: https://www.clarku.edu/\n",
      "  + Added entry #1: Clark University | Challenge Convention. Change Ou...\n",
      "Processing URL 2/50: https://www.clarku.edu/graduate-education/international-students/\n",
      "  + Added entry #2: International Students | Graduate Education\n",
      "  > Found 5 new links, queue size: 109\n",
      "Processing URL 3/50: https://www.clarku.edu/academics/our-faculty/\n",
      "  + Added entry #3: Our Faculty | Clark University\n",
      "  > Found 5 new links, queue size: 113\n",
      "Processing URL 4/50: https://www.clarku.edu/academics/research/science-facilities-and-labs/\n",
      "  + Added entry #4: Science Facilities and Labs | Clark University\n",
      "  > Found 7 new links, queue size: 119\n",
      "Processing URL 5/50: https://www.clarku.edu/academics/undergraduate-curriculum/\n",
      "  + Added entry #5: Undergraduate Curriculum | Clark University\n",
      "  > Found 7 new links, queue size: 125\n",
      "Processing URL 6/50: https://www.clarku.edu/give25/\n",
      "Error fetching https://www.clarku.edu/give25/: 403 Client Error: Forbidden for url: https://www.givecampus.com/campaigns/59762/donations/new?a=9817337\n",
      "  - Failed to fetch content for https://www.clarku.edu/give25/\n",
      "Processing URL 7/50: https://www.clarku.edu/academics/hands-on-learning/\n",
      "  + Added entry #6: Hands-On Learning for Undergraduate Students | Cla...\n",
      "  > Found 3 new links, queue size: 126\n",
      "Processing URL 8/50: https://www.clarku.edu/who-we-are/president-and-leadership/\n",
      "  + Added entry #7: President and Leadership | Clark University\n",
      "  > Found 3 new links, queue size: 128\n",
      "Processing URL 9/50: https://www.clarku.edu/academics/\n",
      "  - Warning: No text content found for https://www.clarku.edu/academics/\n",
      "  + Added entry #8: Academics | Clark University\n",
      "  > Found 5 new links, queue size: 132\n",
      "Processing URL 10/50: https://www.clarku.edu/offices/\n",
      "  + Added entry #9: Offices | Clark University\n",
      "  > Found 51 new links, queue size: 182\n",
      "Processing URL 11/50: https://www.clarku.edu/life-at-clark/\n",
      "  + Added entry #10: Life at Clark | Clark University\n",
      "  > Found 1 new links, queue size: 182\n",
      "Processing URL 12/50: https://www.clarku.edu/undergraduate-admissions/our-process-and-timeline/\n",
      "  + Added entry #11: Application Process - Undergraduate Admissions\n",
      "  > Found 17 new links, queue size: 198\n",
      "Processing URL 13/50: https://www.clarku.edu/parents/\n",
      "  + Added entry #12: For Parents and Families | Clark University\n",
      "  > Found 6 new links, queue size: 203\n",
      "Processing URL 14/50: https://www.clarku.edu/offices/registrar/academic-calendars/\n",
      "  - Warning: No text content found for https://www.clarku.edu/offices/registrar/academic-calendars/\n",
      "  + Added entry #13: Academic Calendars | Registrar's Office | Clark Un...\n",
      "  > Found 5 new links, queue size: 207\n",
      "Processing URL 15/50: https://www.clarku.edu/life-at-clark/athletics-and-recreation/\n",
      "  + Added entry #14: Athletics | Clark University\n",
      "Processing URL 16/50: https://www.clarku.edu/undergraduate-admissions/campus-visits/?ua\n",
      "  + Added entry #15: Visit - Undergraduate Admissions\n",
      "  > Found 2 new links, queue size: 207\n",
      "Processing URL 17/50: https://www.clarku.edu/who-we-are/history-and-traditions/annual-events/\n",
      "  + Added entry #16: Annual Events | Clark University\n",
      "  > Found 4 new links, queue size: 210\n",
      "Processing URL 18/50: https://www.clarku.edu/contactus/\n",
      "  + Added entry #17: Contact Us | Clark University\n",
      "  > Found 1 new links, queue size: 210\n",
      "Processing URL 19/50: https://www.clarku.edu/search?q=human+resources\n",
      "  - Warning: No text content found for https://www.clarku.edu/search?q=human+resources\n",
      "  + Added entry #18: Search | Clark University\n",
      "Processing URL 20/50: https://www.clarku.edu/search?q=courses\n",
      "  - Warning: No text content found for https://www.clarku.edu/search?q=courses\n",
      "  + Added entry #19: Search | Clark University\n",
      "Processing URL 21/50: https://www.clarku.edu/transportation/\n",
      "  + Added entry #20: Transportation - Clark University\n",
      "  > Found 7 new links, queue size: 214\n",
      "Processing URL 22/50: https://www.clarku.edu/undergraduate-admissions/cost-and-financial-aid/\n",
      "  + Added entry #21: Cost and Financial Aid - Undergraduate Admissions\n",
      "  > Found 8 new links, queue size: 221\n",
      "Processing URL 23/50: https://www.clarku.edu/undergraduate-admissions/find-your-counselor/\n",
      "  + Added entry #22: Find Your Admissions Counselor - Undergraduate Adm...\n",
      "Processing URL 24/50: https://www.clarku.edu/offices/registrar/\n",
      "  - Warning: No text content found for https://www.clarku.edu/offices/registrar/\n",
      "  + Added entry #23: Registrar's Office | Clark University\n",
      "  > Found 2 new links, queue size: 221\n",
      "Processing URL 25/50: https://www.clarku.edu/life-at-clark/the-arts-at-clark/\n",
      "  + Added entry #24: The Arts at Clark | Clark University\n",
      "  > Found 6 new links, queue size: 226\n",
      "Processing URL 26/50: https://www.clarku.edu/faculty-staff-resources/\n",
      "  + Added entry #25: Faculty and Staff Resources | Clark University\n",
      "  > Found 21 new links, queue size: 246\n",
      "Processing URL 27/50: https://www.clarku.edu/offices/registrar/courses-and-schedules/\n",
      "  - Warning: No text content found for https://www.clarku.edu/offices/registrar/courses-and-schedules/\n",
      "  + Added entry #26: Courses and Schedules | Registrar's Office | Clark...\n",
      "  > Found 8 new links, queue size: 253\n",
      "Processing URL 28/50: https://www.clarku.edu/offices/sponsored-programs-and-research/\n",
      "  - Warning: No text content found for https://www.clarku.edu/offices/sponsored-programs-and-research/\n",
      "  + Added entry #27: Sponsored Programs and Research | Clark University\n",
      "  > Found 14 new links, queue size: 266\n",
      "Processing URL 29/50: https://www.clarku.edu/undergraduate-admissions/information-for-school-counselors/\n",
      "  + Added entry #28: Information for School Counselors - Undergraduate ...\n",
      "  > Found 1 new links, queue size: 266\n",
      "Processing URL 30/50: https://www.clarku.edu/policies/?wdt_search=privacy\n",
      "  + Added entry #29: Home - University Policies\n",
      "  > Found 7 new links, queue size: 272\n",
      "Processing URL 31/50: https://www.clarku.edu/academics/undergraduate-advising/\n",
      "  + Added entry #30: Undergraduate Advising | Clark University\n",
      "Processing URL 32/50: https://www.clarku.edu/international-center/\n",
      "  + Added entry #31: International Students and Scholars | Clark Univer...\n",
      "  > Found 14 new links, queue size: 284\n",
      "Processing URL 33/50: https://www.clarku.edu/graduate-education/admissions/\n",
      "  + Added entry #32: Graduate Admissions | Graduate Education\n",
      "  > Found 2 new links, queue size: 285\n",
      "Processing URL 34/50: https://www.clarku.edu/undergraduate-admissions/campus-visits/\n",
      "  + Added entry #33: Visit - Undergraduate Admissions\n",
      "Processing URL 35/50: https://www.clarku.edu/who-we-are/diversity-and-inclusion/\n",
      "  + Added entry #34: Diversity and Inclusion | Clark University\n",
      "  > Found 1 new links, queue size: 284\n",
      "Processing URL 36/50: https://www.clarku.edu/who-we-are/our-campus-and-location/worcester/\n",
      "  + Added entry #35: Worcester | Clark University\n",
      "  > Found 1 new links, queue size: 284\n",
      "Processing URL 37/50: https://www.clarku.edu/who-we-are/people-directory/\n",
      "  - Warning: No text content found for https://www.clarku.edu/who-we-are/people-directory/\n",
      "  + Added entry #36: People Directory | Clark University\n",
      "Processing URL 38/50: https://www.clarku.edu/alumni/\n",
      "  + Added entry #37: Alumni | Clark University\n",
      "  > Found 27 new links, queue size: 309\n",
      "Processing URL 39/50: https://www.clarku.edu/graduate-education/graduate-academics/\n",
      "  + Added entry #38: Graduate Academics | Graduate Education\n",
      "  > Found 12 new links, queue size: 320\n",
      "Processing URL 40/50: https://www.clarku.edu/international-students-scholars/\n",
      "  + Added entry #39: International Students and Scholars | Clark Univer...\n",
      "Processing URL 41/50: https://www.clarku.edu\n",
      "  + Added entry #40: Clark University | Challenge Convention. Change Ou...\n",
      "Processing URL 42/50: https://www.clarku.edu/undergraduate-admissions/apply/\n",
      "  + Added entry #41: Apply - Undergraduate Admissions\n",
      "Processing URL 43/50: https://www.clarku.edu/search?q=handshake\n",
      "  - Warning: No text content found for https://www.clarku.edu/search?q=handshake\n",
      "  + Added entry #42: Search | Clark University\n",
      "Processing URL 44/50: https://www.clarku.edu/undergraduate-admissions/virtual-visits/\n",
      "  + Added entry #43: Visit - Undergraduate Admissions\n",
      "Processing URL 45/50: https://www.clarku.edu/who-we-are/our-mission/\n",
      "  + Added entry #44: Our Mission | Clark University\n",
      "Processing URL 46/50: https://www.clarku.edu/who-we-are/fast-facts/\n",
      "  + Added entry #45: Fast Facts | Clark University\n",
      "Processing URL 47/50: https://www.clarku.edu/career-experience/success-after-clark/\n",
      "  + Added entry #46: Success After Clark | Clark University\n",
      "Processing URL 48/50: https://www.clarku.edu/academics/schools-and-departments/\n",
      "  + Added entry #47: Schools and Departments | Clark University\n",
      "  > Found 20 new links, queue size: 331\n",
      "Processing URL 49/50: https://www.clarku.edu/graduate-education/\n",
      "  + Added entry #48: Graduate Education | Clark University\n",
      "  > Found 2 new links, queue size: 332\n",
      "Processing URL 50/50: https://www.clarku.edu/who-we-are/\n",
      "  + Added entry #49: Who We Are | Clark University\n",
      "Processed 50 URLs, extracted 49 content entries\n",
      "Skipped 1 URLs due to errors or no content\n",
      "Found 9 URLs with empty content sections\n",
      "Scraping complete. Data saved to scraped_clark_data.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # List of starting URLs for Clark Universityâ€“related domains\n",
    "    root_url = \"https://www.clarku.edu/\"\n",
    "    \n",
    "    # Initialize data structures\n",
    "    all_entries = []\n",
    "    visited_urls = set()  # To avoid duplicate processing\n",
    "    urls_to_visit = collections.deque([root_url])  # Queue of URLs to process\n",
    "    max_urls = 50  # Increased from 250 to process more URLs\n",
    "    counter = 1  # For generating numerical IDs\n",
    "    skipped_urls = 0\n",
    "    empty_content_urls = 0\n",
    "    \n",
    "    # First get initial set of links from the root page\n",
    "    print(f\"Starting with root URL: {root_url}\")\n",
    "    html = fetch_url(root_url)\n",
    "    if html:\n",
    "        # Extract links from the homepage\n",
    "        first_level_links = extract_links(html, root_url)\n",
    "        print(f\"Found {len(first_level_links)} links on homepage\")\n",
    "        urls_to_visit.extend(first_level_links)\n",
    "    \n",
    "    # Use a breadth-first approach to visit URLs\n",
    "    while urls_to_visit and len(visited_urls) < max_urls:\n",
    "        current_url = urls_to_visit.popleft()  # Using deque for better performance\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "        \n",
    "        # Remove anchor tags for visited_urls tracking but keep original for content extraction\n",
    "        base_url = current_url.split('#')[0]\n",
    "        if base_url in visited_urls:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing URL {len(visited_urls)+1}/{max_urls}: {current_url}\")\n",
    "        visited_urls.add(base_url)\n",
    "        \n",
    "        # Fetch page content\n",
    "        html = fetch_url(current_url)\n",
    "        if not html:\n",
    "            print(f\"  - Failed to fetch content for {current_url}\")\n",
    "            skipped_urls += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract content\n",
    "        try:\n",
    "            title, content = extract_content(html, current_url)\n",
    "            \n",
    "            # Add to entries - only require title OR content, not both\n",
    "            if title or content:\n",
    "                try:\n",
    "                    # Try to parse the content to check if it has actual text\n",
    "                    content_dict = eval(content)\n",
    "                    has_text = any(section.get('text') for section in content_dict.get('sections', []))\n",
    "                    \n",
    "                    if not has_text:\n",
    "                        print(f\"  - Warning: No text content found for {current_url}\")\n",
    "                        empty_content_urls += 1\n",
    "                \n",
    "                    entry = {\n",
    "                        \"id\": str(counter),\n",
    "                        \"title\": title if title else \"Untitled Page\",\n",
    "                        \"url\": current_url,\n",
    "                        \"text\": content\n",
    "                    }\n",
    "                    all_entries.append(entry)\n",
    "                    counter += 1\n",
    "                    print(f\"  + Added entry #{counter-1}: {title[:50]}{'...' if len(title) > 50 else ''}\")\n",
    "                except:\n",
    "                    # If parsing fails, still add the entry\n",
    "                    entry = {\n",
    "                        \"id\": str(counter),\n",
    "                        \"title\": title if title else \"Untitled Page\",\n",
    "                        \"url\": current_url,\n",
    "                        \"text\": content if content else \"Unable to extract content\"\n",
    "                    }\n",
    "                    all_entries.append(entry)\n",
    "                    counter += 1\n",
    "                    print(f\"  + Added entry #{counter-1}: {title[:50]}{'...' if len(title) > 50 else ''}\")\n",
    "            else:\n",
    "                print(f\"  - No content or title found for {current_url}\")\n",
    "                skipped_urls += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ! Error processing content for {current_url}: {e}\")\n",
    "            skipped_urls += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract more links if we need them\n",
    "        if len(visited_urls) < max_urls:\n",
    "            try:\n",
    "                new_links = extract_links(html, current_url)\n",
    "                # Only add links we haven't visited yet\n",
    "                added_links = 0\n",
    "                for link in new_links:\n",
    "                    base_link = link.split('#')[0]\n",
    "                    if base_link not in visited_urls and link not in urls_to_visit:\n",
    "                        urls_to_visit.append(link)\n",
    "                        added_links += 1\n",
    "                \n",
    "                if added_links:\n",
    "                    print(f\"  > Found {added_links} new links, queue size: {len(urls_to_visit)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ! Error extracting links from {current_url}: {e}\")\n",
    "    \n",
    "    print(f\"Processed {len(visited_urls)} URLs, extracted {len(all_entries)} content entries\")\n",
    "    print(f\"Skipped {skipped_urls} URLs due to errors or no content\")\n",
    "    print(f\"Found {empty_content_urls} URLs with empty content sections\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"scraped_clark_data.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_entries, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Scraping complete. Data saved to {output_file}\")\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
