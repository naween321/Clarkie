{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clark University Catalog Link Extractor\n",
    "\n",
    "This notebook extracts all links from the Clark University catalog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL to scrape\n",
    "url = \"https://catalog.clarku.edu/content.php?catoid=34&navoid=2847&print\"\n",
    "\n",
    "# Function to fetch the webpage content\n",
    "def fetch_page(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch the page content\n",
    "html_content = fetch_page(url)\n",
    "\n",
    "if html_content:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(\"Failed to fetch page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 377 links on the page\n"
     ]
    }
   ],
   "source": [
    "# Parse HTML and extract links\n",
    "def extract_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = []\n",
    "    \n",
    "    # Find all anchor tags\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = {\n",
    "            'text': a_tag.text.strip(),\n",
    "            'href': a_tag['href']\n",
    "        }\n",
    "        links.append(link)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Extract links from the page\n",
    "if html_content:\n",
    "    links = extract_links(html_content)\n",
    "    print(f\"Found {len(links)} links on the page\")\n",
    "else:\n",
    "    links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display and analyze the links\n",
    "if links:\n",
    "    df_links = pd.DataFrame(links)\n",
    "    \n",
    "    # Display first 10 links\n",
    "    df_links.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186 potential course/program links\n"
     ]
    }
   ],
   "source": [
    "# Filter links that contain specific patterns (e.g., course links)\n",
    "if links:\n",
    "    # Example: Filter links that might be course-related\n",
    "    course_links = [link for link in links if 'preview_course' in link['href'] or 'preview_program' in link['href']]\n",
    "    \n",
    "    print(f\"Found {len(course_links)} potential course/program links\")\n",
    "    \n",
    "    # Display them as a DataFrame\n",
    "    if course_links:\n",
    "        pd.DataFrame(course_links).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix relative URLs by adding the base URL\n",
    "def fix_relative_urls(links, base_url=\"https://catalog.clarku.edu/\"):\n",
    "    for link in links:\n",
    "        if link['href'] and not (link['href'].startswith('http://') or link['href'].startswith('https://')):\n",
    "            if link['href'].startswith('/'):\n",
    "                link['href'] = base_url.rstrip('/') + link['href']\n",
    "            else:\n",
    "                link['href'] = base_url.rstrip('/') + '/' + link['href']\n",
    "    return links\n",
    "\n",
    "# Fix relative URLs\n",
    "if links:\n",
    "    links = fix_relative_urls(links)\n",
    "    \n",
    "    # Show updated links\n",
    "    pd.DataFrame(links).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links saved to clarku_catalog_links.csv\n"
     ]
    }
   ],
   "source": [
    "# Save links to CSV\n",
    "if links:\n",
    "    df_links = pd.DataFrame(links)\n",
    "    csv_filename = \"clarku_catalog_links.csv\"\n",
    "    df_links.to_csv(csv_filename, index=False)\n",
    "    print(f\"Links saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. Fetched the Clark University catalog page\n",
    "2. Extracted all links from the page\n",
    "3. Fixed relative URLs to absolute URLs\n",
    "4. Saved the links to a CSV file\n",
    "\n",
    "You can further filter or analyze these links as needed for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
